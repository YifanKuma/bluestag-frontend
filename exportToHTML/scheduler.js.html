<html>
<head>
<title>scheduler.js</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #067d17;}
.s1 { color: #080808;}
.s2 { color: #0033b3;}
.s3 { color: #1750eb;}
.s4 { color: #8c8c8c; font-style: italic;}
.s5 { color: #8c8c8c; font-style: italic;}
</style>
</head>
<body bgcolor="#ffffff">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#c0c0c0" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
scheduler.js</font>
</center></td></tr></table>
<pre><span class="s0">&quot;use strict&quot;</span><span class="s1">;</span>
<span class="s1">Object.defineProperty(exports, </span><span class="s0">&quot;__esModule&quot;</span><span class="s1">, {</span>
    <span class="s1">value: </span><span class="s2">true</span>
<span class="s1">});</span>
<span class="s3">0 </span><span class="s1">&amp;&amp; (module.exports = {</span>
    <span class="s1">cancelPrefetchTask: </span><span class="s2">null</span><span class="s1">,</span>
    <span class="s1">isPrefetchTaskDirty: </span><span class="s2">null</span><span class="s1">,</span>
    <span class="s1">pingPrefetchTask: </span><span class="s2">null</span><span class="s1">,</span>
    <span class="s1">reschedulePrefetchTask: </span><span class="s2">null</span><span class="s1">,</span>
    <span class="s1">schedulePrefetchTask: </span><span class="s2">null</span>
<span class="s1">});</span>
<span class="s2">function </span><span class="s1">_export(target, all) {</span>
    <span class="s2">for</span><span class="s1">(</span><span class="s2">var </span><span class="s1">name </span><span class="s2">in </span><span class="s1">all)Object.defineProperty(target, name, {</span>
        <span class="s1">enumerable: </span><span class="s2">true</span><span class="s1">,</span>
        <span class="s1">get: all[name]</span>
    <span class="s1">});</span>
<span class="s1">}</span>
<span class="s1">_export(exports, {</span>
    <span class="s1">cancelPrefetchTask: </span><span class="s2">function</span><span class="s1">() {</span>
        <span class="s2">return </span><span class="s1">cancelPrefetchTask;</span>
    <span class="s1">},</span>
    <span class="s1">isPrefetchTaskDirty: </span><span class="s2">function</span><span class="s1">() {</span>
        <span class="s2">return </span><span class="s1">isPrefetchTaskDirty;</span>
    <span class="s1">},</span>
    <span class="s1">pingPrefetchTask: </span><span class="s2">function</span><span class="s1">() {</span>
        <span class="s2">return </span><span class="s1">pingPrefetchTask;</span>
    <span class="s1">},</span>
    <span class="s1">reschedulePrefetchTask: </span><span class="s2">function</span><span class="s1">() {</span>
        <span class="s2">return </span><span class="s1">reschedulePrefetchTask;</span>
    <span class="s1">},</span>
    <span class="s1">schedulePrefetchTask: </span><span class="s2">function</span><span class="s1">() {</span>
        <span class="s2">return </span><span class="s1">schedulePrefetchTask;</span>
    <span class="s1">}</span>
<span class="s1">});</span>
<span class="s2">const </span><span class="s1">_types = require(</span><span class="s0">&quot;../../../server/app-render/types&quot;</span><span class="s1">);</span>
<span class="s2">const </span><span class="s1">_matchsegments = require(</span><span class="s0">&quot;../match-segments&quot;</span><span class="s1">);</span>
<span class="s2">const </span><span class="s1">_cache = require(</span><span class="s0">&quot;./cache&quot;</span><span class="s1">);</span>
<span class="s2">const </span><span class="s1">_segmentcache = require(</span><span class="s0">&quot;../segment-cache&quot;</span><span class="s1">);</span>
<span class="s2">const </span><span class="s1">_segment = require(</span><span class="s0">&quot;../../../shared/lib/segment&quot;</span><span class="s1">);</span>
<span class="s2">const </span><span class="s1">scheduleMicrotask = </span><span class="s2">typeof </span><span class="s1">queueMicrotask === </span><span class="s0">'function' </span><span class="s1">? queueMicrotask : (fn)=&gt;Promise.resolve().then(fn).catch((error)=&gt;setTimeout(()=&gt;{</span>
            <span class="s2">throw </span><span class="s1">error;</span>
        <span class="s1">}));</span>
<span class="s2">const </span><span class="s1">taskHeap = [];</span>
<span class="s2">let </span><span class="s1">inProgressRequests = </span><span class="s3">0</span><span class="s1">;</span>
<span class="s2">let </span><span class="s1">sortIdCounter = </span><span class="s3">0</span><span class="s1">;</span>
<span class="s2">let </span><span class="s1">didScheduleMicrotask = </span><span class="s2">false</span><span class="s1">;</span>
<span class="s4">// The most recently hovered (or touched, etc) link, i.e. the most recent task</span>
<span class="s4">// scheduled at Intent priority. There's only ever a single task at Intent</span>
<span class="s4">// priority at a time. We reserve special network bandwidth for this task only.</span>
<span class="s2">let </span><span class="s1">mostRecentlyHoveredLink = </span><span class="s2">null</span><span class="s1">;</span>
<span class="s2">function </span><span class="s1">schedulePrefetchTask(key, treeAtTimeOfPrefetch, fetchStrategy, priority, onInvalidate) {</span>
    <span class="s4">// Spawn a new prefetch task</span>
    <span class="s2">const </span><span class="s1">task = {</span>
        <span class="s1">key,</span>
        <span class="s1">treeAtTimeOfPrefetch,</span>
        <span class="s1">cacheVersion: (</span><span class="s3">0</span><span class="s1">, _segmentcache.getCurrentCacheVersion)(),</span>
        <span class="s1">priority,</span>
        <span class="s1">phase: </span><span class="s3">1</span><span class="s1">,</span>
        <span class="s1">hasBackgroundWork: </span><span class="s2">false</span><span class="s1">,</span>
        <span class="s1">fetchStrategy,</span>
        <span class="s1">sortId: sortIdCounter++,</span>
        <span class="s1">isCanceled: </span><span class="s2">false</span><span class="s1">,</span>
        <span class="s1">onInvalidate,</span>
        <span class="s1">_heapIndex: -</span><span class="s3">1</span>
    <span class="s1">};</span>
    <span class="s1">trackMostRecentlyHoveredLink(task);</span>
    <span class="s1">heapPush(taskHeap, task);</span>
    <span class="s4">// Schedule an async task to process the queue.</span>
    <span class="s4">//</span>
    <span class="s4">// The main reason we process the queue in an async task is for batching.</span>
    <span class="s4">// It's common for a single JS task/event to trigger multiple prefetches.</span>
    <span class="s4">// By deferring to a microtask, we only process the queue once per JS task.</span>
    <span class="s4">// If they have different priorities, it also ensures they are processed in</span>
    <span class="s4">// the optimal order.</span>
    <span class="s1">ensureWorkIsScheduled();</span>
    <span class="s2">return </span><span class="s1">task;</span>
<span class="s1">}</span>
<span class="s2">function </span><span class="s1">cancelPrefetchTask(task) {</span>
    <span class="s4">// Remove the prefetch task from the queue. If the task already completed,</span>
    <span class="s4">// then this is a no-op.</span>
    <span class="s4">//</span>
    <span class="s4">// We must also explicitly mark the task as canceled so that a blocked task</span>
    <span class="s4">// does not get added back to the queue when it's pinged by the network.</span>
    <span class="s1">task.isCanceled = </span><span class="s2">true</span><span class="s1">;</span>
    <span class="s1">heapDelete(taskHeap, task);</span>
<span class="s1">}</span>
<span class="s2">function </span><span class="s1">reschedulePrefetchTask(task, treeAtTimeOfPrefetch, fetchStrategy, priority) {</span>
    <span class="s4">// Bump the prefetch task to the top of the queue, as if it were a fresh</span>
    <span class="s4">// task. This is essentially the same as canceling the task and scheduling</span>
    <span class="s4">// a new one, except it reuses the original object.</span>
    <span class="s4">//</span>
    <span class="s4">// The primary use case is to increase the priority of a Link-initated</span>
    <span class="s4">// prefetch on hover.</span>
    <span class="s4">// Un-cancel the task, in case it was previously canceled.</span>
    <span class="s1">task.isCanceled = </span><span class="s2">false</span><span class="s1">;</span>
    <span class="s1">task.phase = </span><span class="s3">1</span><span class="s1">;</span>
    <span class="s4">// Assign a new sort ID to move it ahead of all other tasks at the same</span>
    <span class="s4">// priority level. (Higher sort IDs are processed first.)</span>
    <span class="s1">task.sortId = sortIdCounter++;</span>
    <span class="s1">task.priority = </span><span class="s4">// If this task is the most recently hovered link, maintain its</span>
    <span class="s4">// Intent priority, even if the rescheduled priority is lower.</span>
    <span class="s1">task === mostRecentlyHoveredLink ? _segmentcache.PrefetchPriority.Intent : priority;</span>
    <span class="s1">task.treeAtTimeOfPrefetch = treeAtTimeOfPrefetch;</span>
    <span class="s1">task.fetchStrategy = fetchStrategy;</span>
    <span class="s1">trackMostRecentlyHoveredLink(task);</span>
    <span class="s2">if </span><span class="s1">(task._heapIndex !== -</span><span class="s3">1</span><span class="s1">) {</span>
        <span class="s4">// The task is already in the queue.</span>
        <span class="s1">heapResift(taskHeap, task);</span>
    <span class="s1">} </span><span class="s2">else </span><span class="s1">{</span>
        <span class="s1">heapPush(taskHeap, task);</span>
    <span class="s1">}</span>
    <span class="s1">ensureWorkIsScheduled();</span>
<span class="s1">}</span>
<span class="s2">function </span><span class="s1">isPrefetchTaskDirty(task, nextUrl, tree) {</span>
    <span class="s4">// This is used to quickly bail out of a prefetch task if the result is</span>
    <span class="s4">// guaranteed to not have changed since the task was initiated. This is</span>
    <span class="s4">// strictly an optimization — theoretically, if it always returned true, no</span>
    <span class="s4">// behavior should change because a full prefetch task will effectively</span>
    <span class="s4">// perform the same checks.</span>
    <span class="s2">const </span><span class="s1">currentCacheVersion = (</span><span class="s3">0</span><span class="s1">, _segmentcache.getCurrentCacheVersion)();</span>
    <span class="s2">return </span><span class="s1">task.cacheVersion !== currentCacheVersion || task.treeAtTimeOfPrefetch !== tree || task.key.nextUrl !== nextUrl;</span>
<span class="s1">}</span>
<span class="s2">function </span><span class="s1">trackMostRecentlyHoveredLink(task) {</span>
    <span class="s4">// Track the mostly recently hovered link, i.e. the most recently scheduled</span>
    <span class="s4">// task at Intent priority. There must only be one such task at a time.</span>
    <span class="s2">if </span><span class="s1">(task.priority === _segmentcache.PrefetchPriority.Intent &amp;&amp; task !== mostRecentlyHoveredLink) {</span>
        <span class="s2">if </span><span class="s1">(mostRecentlyHoveredLink !== </span><span class="s2">null</span><span class="s1">) {</span>
            <span class="s4">// Bump the previously hovered link's priority down to Default.</span>
            <span class="s2">if </span><span class="s1">(mostRecentlyHoveredLink.priority !== _segmentcache.PrefetchPriority.Background) {</span>
                <span class="s1">mostRecentlyHoveredLink.priority = _segmentcache.PrefetchPriority.Default;</span>
                <span class="s1">heapResift(taskHeap, mostRecentlyHoveredLink);</span>
            <span class="s1">}</span>
        <span class="s1">}</span>
        <span class="s1">mostRecentlyHoveredLink = task;</span>
    <span class="s1">}</span>
<span class="s1">}</span>
<span class="s2">function </span><span class="s1">ensureWorkIsScheduled() {</span>
    <span class="s2">if </span><span class="s1">(didScheduleMicrotask) {</span>
        <span class="s4">// Already scheduled a task to process the queue</span>
        <span class="s2">return</span><span class="s1">;</span>
    <span class="s1">}</span>
    <span class="s1">didScheduleMicrotask = </span><span class="s2">true</span><span class="s1">;</span>
    <span class="s1">scheduleMicrotask(processQueueInMicrotask);</span>
<span class="s1">}</span>
<span class="s4">/**</span>
 <span class="s4">* Checks if we've exceeded the maximum number of concurrent prefetch requests,</span>
 <span class="s4">* to avoid saturating the browser's internal network queue. This is a</span>
 <span class="s4">* cooperative limit — prefetch tasks should check this before issuing</span>
 <span class="s4">* new requests.</span>
 <span class="s4">*/ </span><span class="s2">function </span><span class="s1">hasNetworkBandwidth(task) {</span>
    <span class="s4">// TODO: Also check if there's an in-progress navigation. We should never</span>
    <span class="s4">// add prefetch requests to the network queue if an actual navigation is</span>
    <span class="s4">// taking place, to ensure there's sufficient bandwidth for render-blocking</span>
    <span class="s4">// data and resources.</span>
    <span class="s4">// TODO: Consider reserving some amount of bandwidth for static prefetches.</span>
    <span class="s2">if </span><span class="s1">(task.priority === _segmentcache.PrefetchPriority.Intent) {</span>
        <span class="s4">// The most recently hovered link is allowed to exceed the default limit.</span>
        <span class="s4">//</span>
        <span class="s4">// The goal is to always have enough bandwidth to start a new prefetch</span>
        <span class="s4">// request when hovering over a link.</span>
        <span class="s4">//</span>
        <span class="s4">// However, because we don't abort in-progress requests, it's still possible</span>
        <span class="s4">// we'll run out of bandwidth. When links are hovered in quick succession,</span>
        <span class="s4">// there could be multiple hover requests running simultaneously.</span>
        <span class="s2">return </span><span class="s1">inProgressRequests &lt; </span><span class="s3">12</span><span class="s1">;</span>
    <span class="s1">}</span>
    <span class="s4">// The default limit is lower than the limit for a hovered link.</span>
    <span class="s2">return </span><span class="s1">inProgressRequests &lt; </span><span class="s3">4</span><span class="s1">;</span>
<span class="s1">}</span>
<span class="s2">function </span><span class="s1">spawnPrefetchSubtask(prefetchSubtask) {</span>
    <span class="s4">// When the scheduler spawns an async task, we don't await its result.</span>
    <span class="s4">// Instead, the async task writes its result directly into the cache, then</span>
    <span class="s4">// pings the scheduler to continue.</span>
    <span class="s4">//</span>
    <span class="s4">// We process server responses streamingly, so the prefetch subtask will</span>
    <span class="s4">// likely resolve before we're finished receiving all the data. The subtask</span>
    <span class="s4">// result includes a promise that resolves once the network connection is</span>
    <span class="s4">// closed. The scheduler uses this to control network bandwidth by tracking</span>
    <span class="s4">// and limiting the number of concurrent requests.</span>
    <span class="s1">inProgressRequests++;</span>
    <span class="s2">return </span><span class="s1">prefetchSubtask.then((result)=&gt;{</span>
        <span class="s2">if </span><span class="s1">(result === </span><span class="s2">null</span><span class="s1">) {</span>
            <span class="s4">// The prefetch task errored before it could start processing the</span>
            <span class="s4">// network stream. Assume the connection is closed.</span>
            <span class="s1">onPrefetchConnectionClosed();</span>
            <span class="s2">return null</span><span class="s1">;</span>
        <span class="s1">}</span>
        <span class="s4">// Wait for the connection to close before freeing up more bandwidth.</span>
        <span class="s1">result.closed.then(onPrefetchConnectionClosed);</span>
        <span class="s2">return </span><span class="s1">result.value;</span>
    <span class="s1">});</span>
<span class="s1">}</span>
<span class="s2">function </span><span class="s1">onPrefetchConnectionClosed() {</span>
    <span class="s1">inProgressRequests--;</span>
    <span class="s4">// Notify the scheduler that we have more bandwidth, and can continue</span>
    <span class="s4">// processing tasks.</span>
    <span class="s1">ensureWorkIsScheduled();</span>
<span class="s1">}</span>
<span class="s2">function </span><span class="s1">pingPrefetchTask(task) {</span>
    <span class="s4">// &quot;Ping&quot; a prefetch that's already in progress to notify it of new data.</span>
    <span class="s2">if </span><span class="s1">(</span><span class="s4">// Check if prefetch was canceled.</span>
    <span class="s1">task.isCanceled || </span><span class="s4">// Check if prefetch is already queued.</span>
    <span class="s1">task._heapIndex !== -</span><span class="s3">1</span><span class="s1">) {</span>
        <span class="s2">return</span><span class="s1">;</span>
    <span class="s1">}</span>
    <span class="s4">// Add the task back to the queue.</span>
    <span class="s1">heapPush(taskHeap, task);</span>
    <span class="s1">ensureWorkIsScheduled();</span>
<span class="s1">}</span>
<span class="s2">function </span><span class="s1">processQueueInMicrotask() {</span>
    <span class="s1">didScheduleMicrotask = </span><span class="s2">false</span><span class="s1">;</span>
    <span class="s4">// We aim to minimize how often we read the current time. Since nearly all</span>
    <span class="s4">// functions in the prefetch scheduler are synchronous, we can read the time</span>
    <span class="s4">// once and pass it as an argument wherever it's needed.</span>
    <span class="s2">const </span><span class="s1">now = Date.now();</span>
    <span class="s4">// Process the task queue until we run out of network bandwidth.</span>
    <span class="s2">let </span><span class="s1">task = heapPeek(taskHeap);</span>
    <span class="s2">while</span><span class="s1">(task !== </span><span class="s2">null </span><span class="s1">&amp;&amp; hasNetworkBandwidth(task)){</span>
        <span class="s1">task.cacheVersion = (</span><span class="s3">0</span><span class="s1">, _segmentcache.getCurrentCacheVersion)();</span>
        <span class="s2">const </span><span class="s1">route = (</span><span class="s3">0</span><span class="s1">, _cache.readOrCreateRouteCacheEntry)(now, task);</span>
        <span class="s2">const </span><span class="s1">exitStatus = pingRootRouteTree(now, task, route);</span>
        <span class="s4">// The `hasBackgroundWork` field is only valid for a single attempt. Reset</span>
        <span class="s4">// it immediately upon exit.</span>
        <span class="s2">const </span><span class="s1">hasBackgroundWork = task.hasBackgroundWork;</span>
        <span class="s1">task.hasBackgroundWork = </span><span class="s2">false</span><span class="s1">;</span>
        <span class="s2">switch</span><span class="s1">(exitStatus){</span>
            <span class="s2">case </span><span class="s3">0</span><span class="s1">:</span>
                <span class="s4">// The task yielded because there are too many requests in progress.</span>
                <span class="s4">// Stop processing tasks until we have more bandwidth.</span>
                <span class="s2">return</span><span class="s1">;</span>
            <span class="s2">case </span><span class="s3">1</span><span class="s1">:</span>
                <span class="s4">// The task is blocked. It needs more data before it can proceed.</span>
                <span class="s4">// Keep the task out of the queue until the server responds.</span>
                <span class="s1">heapPop(taskHeap);</span>
                <span class="s4">// Continue to the next task</span>
                <span class="s1">task = heapPeek(taskHeap);</span>
                <span class="s2">continue</span><span class="s1">;</span>
            <span class="s2">case </span><span class="s3">2</span><span class="s1">:</span>
                <span class="s2">if </span><span class="s1">(task.phase === </span><span class="s3">1</span><span class="s1">) {</span>
                    <span class="s4">// Finished prefetching the route tree. Proceed to prefetching</span>
                    <span class="s4">// the segments.</span>
                    <span class="s1">task.phase = </span><span class="s3">0</span><span class="s1">;</span>
                    <span class="s1">heapResift(taskHeap, task);</span>
                <span class="s1">} </span><span class="s2">else if </span><span class="s1">(hasBackgroundWork) {</span>
                    <span class="s4">// The task spawned additional background work. Reschedule the task</span>
                    <span class="s4">// at background priority.</span>
                    <span class="s1">task.priority = _segmentcache.PrefetchPriority.Background;</span>
                    <span class="s1">heapResift(taskHeap, task);</span>
                <span class="s1">} </span><span class="s2">else </span><span class="s1">{</span>
                    <span class="s4">// The prefetch is complete. Continue to the next task.</span>
                    <span class="s1">heapPop(taskHeap);</span>
                <span class="s1">}</span>
                <span class="s1">task = heapPeek(taskHeap);</span>
                <span class="s2">continue</span><span class="s1">;</span>
            <span class="s2">default</span><span class="s1">:</span>
                <span class="s1">exitStatus;</span>
        <span class="s1">}</span>
    <span class="s1">}</span>
<span class="s1">}</span>
<span class="s4">/**</span>
 <span class="s4">* Check this during a prefetch task to determine if background work can be</span>
 <span class="s4">* performed. If so, it evaluates to `true`. Otherwise, it returns `false`,</span>
 <span class="s4">* while also scheduling a background task to run later. Usage:</span>
 <span class="s4">*</span>
 <span class="s4">* </span><span class="s5">@example</span>
 <span class="s4">* if (background(task)) {</span>
 <span class="s4">*   // Perform background-pri work</span>
 <span class="s4">* }</span>
 <span class="s4">*/ </span><span class="s2">function </span><span class="s1">background(task) {</span>
    <span class="s2">if </span><span class="s1">(task.priority === _segmentcache.PrefetchPriority.Background) {</span>
        <span class="s2">return true</span><span class="s1">;</span>
    <span class="s1">}</span>
    <span class="s1">task.hasBackgroundWork = </span><span class="s2">true</span><span class="s1">;</span>
    <span class="s2">return false</span><span class="s1">;</span>
<span class="s1">}</span>
<span class="s2">function </span><span class="s1">pingRootRouteTree(now, task, route) {</span>
    <span class="s2">switch</span><span class="s1">(route.status){</span>
        <span class="s2">case </span><span class="s1">_cache.EntryStatus.Empty:</span>
            <span class="s1">{</span>
                <span class="s4">// Route is not yet cached, and there's no request already in progress.</span>
                <span class="s4">// Spawn a task to request the route, load it into the cache, and ping</span>
                <span class="s4">// the task to continue.</span>
                <span class="s4">// TODO: There are multiple strategies in the &lt;Link&gt; API for prefetching</span>
                <span class="s4">// a route. Currently we've only implemented the main one: per-segment,</span>
                <span class="s4">// static-data only.</span>
                <span class="s4">//</span>
                <span class="s4">// There's also `&lt;Link prefetch={true}&gt;`</span>
                <span class="s4">// which prefetch both static *and* dynamic data.</span>
                <span class="s4">// Similarly, we need to fallback to the old, per-page</span>
                <span class="s4">// behavior if PPR is disabled for a route (via the incremental opt-in).</span>
                <span class="s4">//</span>
                <span class="s4">// Those cases will be handled here.</span>
                <span class="s1">spawnPrefetchSubtask((</span><span class="s3">0</span><span class="s1">, _cache.fetchRouteOnCacheMiss)(route, task));</span>
                <span class="s4">// If the request takes longer than a minute, a subsequent request should</span>
                <span class="s4">// retry instead of waiting for this one. When the response is received,</span>
                <span class="s4">// this value will be replaced by a new value based on the stale time sent</span>
                <span class="s4">// from the server.</span>
                <span class="s4">// TODO: We should probably also manually abort the fetch task, to reclaim</span>
                <span class="s4">// server bandwidth.</span>
                <span class="s1">route.staleAt = now + </span><span class="s3">60 </span><span class="s1">* </span><span class="s3">1000</span><span class="s1">;</span>
                <span class="s4">// Upgrade to Pending so we know there's already a request in progress</span>
                <span class="s1">route.status = _cache.EntryStatus.Pending;</span>
            <span class="s4">// Intentional fallthrough to the Pending branch</span>
            <span class="s1">}</span>
        <span class="s2">case </span><span class="s1">_cache.EntryStatus.Pending:</span>
            <span class="s1">{</span>
                <span class="s4">// Still pending. We can't start prefetching the segments until the route</span>
                <span class="s4">// tree has loaded. Add the task to the set of blocked tasks so that it</span>
                <span class="s4">// is notified when the route tree is ready.</span>
                <span class="s2">const </span><span class="s1">blockedTasks = route.blockedTasks;</span>
                <span class="s2">if </span><span class="s1">(blockedTasks === </span><span class="s2">null</span><span class="s1">) {</span>
                    <span class="s1">route.blockedTasks = </span><span class="s2">new </span><span class="s1">Set([</span>
                        <span class="s1">task</span>
                    <span class="s1">]);</span>
                <span class="s1">} </span><span class="s2">else </span><span class="s1">{</span>
                    <span class="s1">blockedTasks.add(task);</span>
                <span class="s1">}</span>
                <span class="s2">return </span><span class="s3">1</span><span class="s1">;</span>
            <span class="s1">}</span>
        <span class="s2">case </span><span class="s1">_cache.EntryStatus.Rejected:</span>
            <span class="s1">{</span>
                <span class="s4">// Route tree failed to load. Treat as a 404.</span>
                <span class="s2">return </span><span class="s3">2</span><span class="s1">;</span>
            <span class="s1">}</span>
        <span class="s2">case </span><span class="s1">_cache.EntryStatus.Fulfilled:</span>
            <span class="s1">{</span>
                <span class="s2">if </span><span class="s1">(task.phase !== </span><span class="s3">0</span><span class="s1">) {</span>
                    <span class="s4">// Do not prefetch segment data until we've entered the segment phase.</span>
                    <span class="s2">return </span><span class="s3">2</span><span class="s1">;</span>
                <span class="s1">}</span>
                <span class="s4">// Recursively fill in the segment tree.</span>
                <span class="s2">if </span><span class="s1">(!hasNetworkBandwidth(task)) {</span>
                    <span class="s4">// Stop prefetching segments until there's more bandwidth.</span>
                    <span class="s2">return </span><span class="s3">0</span><span class="s1">;</span>
                <span class="s1">}</span>
                <span class="s2">const </span><span class="s1">tree = route.tree;</span>
                <span class="s4">// A task's fetch strategy gets set to `PPR` for any &quot;auto&quot; prefetch.</span>
                <span class="s4">// If it turned out that the route isn't PPR-enabled, we need to use `LoadingBoundary` instead.</span>
                <span class="s4">// We don't need to do this for runtime prefetches, because those are only available in</span>
                <span class="s4">// `cacheComponents`, where every route is PPR.</span>
                <span class="s2">const </span><span class="s1">fetchStrategy = task.fetchStrategy === _segmentcache.FetchStrategy.PPR ? route.isPPREnabled ? _segmentcache.FetchStrategy.PPR : _segmentcache.FetchStrategy.LoadingBoundary : task.fetchStrategy;</span>
                <span class="s2">switch</span><span class="s1">(fetchStrategy){</span>
                    <span class="s2">case </span><span class="s1">_segmentcache.FetchStrategy.PPR:</span>
                        <span class="s4">// Individually prefetch the static shell for each segment. This is</span>
                        <span class="s4">// the default prefetching behavior for static routes, or when PPR is</span>
                        <span class="s4">// enabled. It will not include any dynamic data.</span>
                        <span class="s2">return </span><span class="s1">pingPPRRouteTree(now, task, route, tree);</span>
                    <span class="s2">case </span><span class="s1">_segmentcache.FetchStrategy.Full:</span>
                    <span class="s2">case </span><span class="s1">_segmentcache.FetchStrategy.PPRRuntime:</span>
                    <span class="s2">case </span><span class="s1">_segmentcache.FetchStrategy.LoadingBoundary:</span>
                        <span class="s1">{</span>
                            <span class="s4">// Prefetch multiple segments using a single dynamic request.</span>
                            <span class="s2">const </span><span class="s1">spawnedEntries = </span><span class="s2">new </span><span class="s1">Map();</span>
                            <span class="s2">const </span><span class="s1">dynamicRequestTree = diffRouteTreeAgainstCurrent(now, task, route, task.treeAtTimeOfPrefetch, tree, spawnedEntries, fetchStrategy);</span>
                            <span class="s2">let </span><span class="s1">needsDynamicRequest = spawnedEntries.size &gt; </span><span class="s3">0</span><span class="s1">;</span>
                            <span class="s2">if </span><span class="s1">(!needsDynamicRequest &amp;&amp; route.isHeadPartial &amp;&amp; route.TODO_metadataStatus === _cache.EntryStatus.Empty) {</span>
                                <span class="s4">// All the segment data is already cached, however, we need to issue</span>
                                <span class="s4">// a request anyway so we can prefetch the head. Update the status</span>
                                <span class="s4">// field to prevent additional requests from being spawned while</span>
                                <span class="s4">// this one is in progress.</span>
                                <span class="s4">// TODO: This is a temporary, targeted solution to fix a regression</span>
                                <span class="s4">// we found. It exists to prevent the scheduler from sending a</span>
                                <span class="s4">// redundant request if there's already one in progress.</span>
                                <span class="s4">// Essentially, it will attempt once at most, then give up until the</span>
                                <span class="s4">// route entry expires or is evicted by other means. But because</span>
                                <span class="s4">// this doesn't have its own stale time separate from the route</span>
                                <span class="s4">// itself, there will be edge cases where the metadata fails to be</span>
                                <span class="s4">// fully prefetched. Consider caching metadata using a separate</span>
                                <span class="s4">// entry type so we can model this more cleanly. The circumstances</span>
                                <span class="s4">// that lead to this branch running in the first place are</span>
                                <span class="s4">// relatively rare, so it's not critical.</span>
                                <span class="s1">route.TODO_metadataStatus = _cache.EntryStatus.Fulfilled;</span>
                                <span class="s1">needsDynamicRequest = </span><span class="s2">true</span><span class="s1">;</span>
                                <span class="s4">// This instructs the server to only send the metadata.</span>
                                <span class="s1">dynamicRequestTree[</span><span class="s3">3</span><span class="s1">] = </span><span class="s0">'metadata-only'</span><span class="s1">;</span>
                                <span class="s4">// We can null out the children to reduce the request size, since</span>
                                <span class="s4">// they won't be needed.</span>
                                <span class="s1">dynamicRequestTree[</span><span class="s3">1</span><span class="s1">] = {};</span>
                            <span class="s1">}</span>
                            <span class="s2">if </span><span class="s1">(needsDynamicRequest) {</span>
                                <span class="s4">// Perform a dynamic prefetch request and populate the cache with</span>
                                <span class="s4">// the result</span>
                                <span class="s1">spawnPrefetchSubtask((</span><span class="s3">0</span><span class="s1">, _cache.fetchSegmentPrefetchesUsingDynamicRequest)(task, route, fetchStrategy, dynamicRequestTree, spawnedEntries));</span>
                            <span class="s1">}</span>
                            <span class="s2">return </span><span class="s3">2</span><span class="s1">;</span>
                        <span class="s1">}</span>
                    <span class="s2">default</span><span class="s1">:</span>
                        <span class="s1">fetchStrategy;</span>
                <span class="s1">}</span>
                <span class="s2">break</span><span class="s1">;</span>
            <span class="s1">}</span>
        <span class="s2">default</span><span class="s1">:</span>
            <span class="s1">{</span>
                <span class="s1">route;</span>
            <span class="s1">}</span>
    <span class="s1">}</span>
    <span class="s2">return </span><span class="s3">2</span><span class="s1">;</span>
<span class="s1">}</span>
<span class="s2">function </span><span class="s1">pingPPRRouteTree(now, task, route, tree) {</span>
    <span class="s2">const </span><span class="s1">segment = (</span><span class="s3">0</span><span class="s1">, _cache.readOrCreateSegmentCacheEntry)(now, task, route, tree.cacheKey);</span>
    <span class="s1">pingPerSegment(now, task, route, segment, task.key, tree);</span>
    <span class="s2">if </span><span class="s1">(tree.slots !== </span><span class="s2">null</span><span class="s1">) {</span>
        <span class="s2">if </span><span class="s1">(!hasNetworkBandwidth(task)) {</span>
            <span class="s4">// Stop prefetching segments until there's more bandwidth.</span>
            <span class="s2">return </span><span class="s3">0</span><span class="s1">;</span>
        <span class="s1">}</span>
        <span class="s4">// Recursively ping the children.</span>
        <span class="s2">for</span><span class="s1">(</span><span class="s2">const </span><span class="s1">parallelRouteKey </span><span class="s2">in </span><span class="s1">tree.slots){</span>
            <span class="s2">const </span><span class="s1">childTree = tree.slots[parallelRouteKey];</span>
            <span class="s2">const </span><span class="s1">childExitStatus = pingPPRRouteTree(now, task, route, childTree);</span>
            <span class="s2">if </span><span class="s1">(childExitStatus === </span><span class="s3">0</span><span class="s1">) {</span>
                <span class="s4">// Child yielded without finishing.</span>
                <span class="s2">return </span><span class="s3">0</span><span class="s1">;</span>
            <span class="s1">}</span>
        <span class="s1">}</span>
    <span class="s1">}</span>
    <span class="s4">// This segment and all its children have finished prefetching.</span>
    <span class="s2">return </span><span class="s3">2</span><span class="s1">;</span>
<span class="s1">}</span>
<span class="s2">function </span><span class="s1">diffRouteTreeAgainstCurrent(now, task, route, oldTree, newTree, spawnedEntries, fetchStrategy) {</span>
    <span class="s4">// This is a single recursive traversal that does multiple things:</span>
    <span class="s4">// - Finds the parts of the target route (newTree) that are not part of</span>
    <span class="s4">//   of the current page (oldTree) by diffing them, using the same algorithm</span>
    <span class="s4">//   as a real navigation.</span>
    <span class="s4">// - Constructs a request tree (FlightRouterState) that describes which</span>
    <span class="s4">//   segments need to be prefetched and which ones are already cached.</span>
    <span class="s4">// - Creates a set of pending cache entries for the segments that need to</span>
    <span class="s4">//   be prefetched, so that a subsequent prefetch task does not request the</span>
    <span class="s4">//   same segments again.</span>
    <span class="s2">const </span><span class="s1">oldTreeChildren = oldTree[</span><span class="s3">1</span><span class="s1">];</span>
    <span class="s2">const </span><span class="s1">newTreeChildren = newTree.slots;</span>
    <span class="s2">let </span><span class="s1">requestTreeChildren = {};</span>
    <span class="s2">if </span><span class="s1">(newTreeChildren !== </span><span class="s2">null</span><span class="s1">) {</span>
        <span class="s2">for</span><span class="s1">(</span><span class="s2">const </span><span class="s1">parallelRouteKey </span><span class="s2">in </span><span class="s1">newTreeChildren){</span>
            <span class="s2">const </span><span class="s1">newTreeChild = newTreeChildren[parallelRouteKey];</span>
            <span class="s2">const </span><span class="s1">newTreeChildSegment = newTreeChild.segment;</span>
            <span class="s2">const </span><span class="s1">oldTreeChild = oldTreeChildren[parallelRouteKey];</span>
            <span class="s2">const </span><span class="s1">oldTreeChildSegment = oldTreeChild == </span><span class="s2">null </span><span class="s1">? </span><span class="s2">void </span><span class="s3">0 </span><span class="s1">: oldTreeChild[</span><span class="s3">0</span><span class="s1">];</span>
            <span class="s2">if </span><span class="s1">(oldTreeChildSegment !== undefined &amp;&amp; doesCurrentSegmentMatchCachedSegment(route, newTreeChildSegment, oldTreeChildSegment)) {</span>
                <span class="s4">// This segment is already part of the current route. Keep traversing.</span>
                <span class="s2">const </span><span class="s1">requestTreeChild = diffRouteTreeAgainstCurrent(now, task, route, oldTreeChild, newTreeChild, spawnedEntries, fetchStrategy);</span>
                <span class="s1">requestTreeChildren[parallelRouteKey] = requestTreeChild;</span>
            <span class="s1">} </span><span class="s2">else </span><span class="s1">{</span>
                <span class="s4">// This segment is not part of the current route. We're entering a</span>
                <span class="s4">// part of the tree that we need to prefetch (unless everything is</span>
                <span class="s4">// already cached).</span>
                <span class="s2">switch</span><span class="s1">(fetchStrategy){</span>
                    <span class="s2">case </span><span class="s1">_segmentcache.FetchStrategy.LoadingBoundary:</span>
                        <span class="s1">{</span>
                            <span class="s4">// When PPR is disabled, we can't prefetch per segment. We must</span>
                            <span class="s4">// fallback to the old prefetch behavior and send a dynamic request.</span>
                            <span class="s4">// Only routes that include a loading boundary can be prefetched in</span>
                            <span class="s4">// this way.</span>
                            <span class="s4">//</span>
                            <span class="s4">// This is simlar to a &quot;full&quot; prefetch, but we're much more</span>
                            <span class="s4">// conservative about which segments to include in the request.</span>
                            <span class="s4">//</span>
                            <span class="s4">// The server will only render up to the first loading boundary</span>
                            <span class="s4">// inside new part of the tree. If there's no loading boundary</span>
                            <span class="s4">// anywhere in the tree, the server will never return any data, so</span>
                            <span class="s4">// we can skip the request.</span>
                            <span class="s2">const </span><span class="s1">subtreeHasLoadingBoundary = newTreeChild.hasLoadingBoundary !== _types.HasLoadingBoundary.SubtreeHasNoLoadingBoundary;</span>
                            <span class="s2">const </span><span class="s1">requestTreeChild = subtreeHasLoadingBoundary ? pingPPRDisabledRouteTreeUpToLoadingBoundary(now, task, route, newTreeChild, </span><span class="s2">null</span><span class="s1">, spawnedEntries) : (</span><span class="s3">0</span><span class="s1">, _cache.convertRouteTreeToFlightRouterState)(newTreeChild);</span>
                            <span class="s1">requestTreeChildren[parallelRouteKey] = requestTreeChild;</span>
                            <span class="s2">break</span><span class="s1">;</span>
                        <span class="s1">}</span>
                    <span class="s2">case </span><span class="s1">_segmentcache.FetchStrategy.PPRRuntime:</span>
                        <span class="s1">{</span>
                            <span class="s4">// This is a runtime prefetch. Fetch all cacheable data in the tree,</span>
                            <span class="s4">// not just the static PPR shell.</span>
                            <span class="s2">const </span><span class="s1">requestTreeChild = pingRouteTreeAndIncludeDynamicData(now, task, route, newTreeChild, </span><span class="s2">false</span><span class="s1">, spawnedEntries, fetchStrategy);</span>
                            <span class="s1">requestTreeChildren[parallelRouteKey] = requestTreeChild;</span>
                            <span class="s2">break</span><span class="s1">;</span>
                        <span class="s1">}</span>
                    <span class="s2">case </span><span class="s1">_segmentcache.FetchStrategy.Full:</span>
                        <span class="s1">{</span>
                            <span class="s4">// This is a &quot;full&quot; prefetch. Fetch all the data in the tree, both</span>
                            <span class="s4">// static and dynamic. We issue roughly the same request that we</span>
                            <span class="s4">// would during a real navigation. The goal is that once the</span>
                            <span class="s4">// navigation occurs, the router should not have to fetch any</span>
                            <span class="s4">// additional data.</span>
                            <span class="s4">//</span>
                            <span class="s4">// Although the response will include dynamic data, opting into a</span>
                            <span class="s4">// Full prefetch — via &lt;Link prefetch={true}&gt; — implicitly</span>
                            <span class="s4">// instructs the cache to treat the response as &quot;static&quot;, or non-</span>
                            <span class="s4">// dynamic, since the whole point is to cache it for</span>
                            <span class="s4">// future navigations.</span>
                            <span class="s4">//</span>
                            <span class="s4">// Construct a tree (currently a FlightRouterState) that represents</span>
                            <span class="s4">// which segments need to be prefetched and which ones are already</span>
                            <span class="s4">// cached. If the tree is empty, then we can exit. Otherwise, we'll</span>
                            <span class="s4">// send the request tree to the server and use the response to</span>
                            <span class="s4">// populate the segment cache.</span>
                            <span class="s2">const </span><span class="s1">requestTreeChild = pingRouteTreeAndIncludeDynamicData(now, task, route, newTreeChild, </span><span class="s2">false</span><span class="s1">, spawnedEntries, fetchStrategy);</span>
                            <span class="s1">requestTreeChildren[parallelRouteKey] = requestTreeChild;</span>
                            <span class="s2">break</span><span class="s1">;</span>
                        <span class="s1">}</span>
                    <span class="s2">default</span><span class="s1">:</span>
                        <span class="s1">fetchStrategy;</span>
                <span class="s1">}</span>
            <span class="s1">}</span>
        <span class="s1">}</span>
    <span class="s1">}</span>
    <span class="s2">const </span><span class="s1">requestTree = [</span>
        <span class="s1">newTree.segment,</span>
        <span class="s1">requestTreeChildren,</span>
        <span class="s2">null</span><span class="s1">,</span>
        <span class="s2">null</span><span class="s1">,</span>
        <span class="s1">newTree.isRootLayout</span>
    <span class="s1">];</span>
    <span class="s2">return </span><span class="s1">requestTree;</span>
<span class="s1">}</span>
<span class="s2">function </span><span class="s1">pingPPRDisabledRouteTreeUpToLoadingBoundary(now, task, route, tree, refetchMarkerContext, spawnedEntries) {</span>
    <span class="s4">// This function is similar to pingRouteTreeAndIncludeDynamicData, except the</span>
    <span class="s4">// server is only going to return a minimal loading state — it will stop</span>
    <span class="s4">// rendering at the first loading boundary. Whereas a Full prefetch is</span>
    <span class="s4">// intentionally aggressive and tries to pretfetch all the data that will be</span>
    <span class="s4">// needed for a navigation, a LoadingBoundary prefetch is much more</span>
    <span class="s4">// conservative. For example, it will omit from the request tree any segment</span>
    <span class="s4">// that is already cached, regardles of whether it's partial or full. By</span>
    <span class="s4">// contrast, a Full prefetch will refetch partial segments.</span>
    <span class="s4">// &quot;inside-shared-layout&quot; tells the server where to start looking for a</span>
    <span class="s4">// loading boundary.</span>
    <span class="s2">let </span><span class="s1">refetchMarker = refetchMarkerContext === </span><span class="s2">null </span><span class="s1">? </span><span class="s0">'inside-shared-layout' </span><span class="s1">: </span><span class="s2">null</span><span class="s1">;</span>
    <span class="s2">const </span><span class="s1">segment = (</span><span class="s3">0</span><span class="s1">, _cache.readOrCreateSegmentCacheEntry)(now, task, route, tree.cacheKey);</span>
    <span class="s2">switch</span><span class="s1">(segment.status){</span>
        <span class="s2">case </span><span class="s1">_cache.EntryStatus.Empty:</span>
            <span class="s1">{</span>
                <span class="s4">// This segment is not cached. Add a refetch marker so the server knows</span>
                <span class="s4">// to start rendering here.</span>
                <span class="s4">// TODO: Instead of a &quot;refetch&quot; marker, we could just omit this subtree's</span>
                <span class="s4">// FlightRouterState from the request tree. I think this would probably</span>
                <span class="s4">// already work even without any updates to the server. For consistency,</span>
                <span class="s4">// though, I'll send the full tree and we'll look into this later as part</span>
                <span class="s4">// of a larger redesign of the request protocol.</span>
                <span class="s4">// Add the pending cache entry to the result map.</span>
                <span class="s1">spawnedEntries.set(tree.cacheKey, (</span><span class="s3">0</span><span class="s1">, _cache.upgradeToPendingSegment)(segment, </span><span class="s4">// Set the fetch strategy to LoadingBoundary to indicate that the server</span>
                <span class="s4">// might not include it in the pending response. If another route is able</span>
                <span class="s4">// to issue a per-segment request, we'll do that in the background.</span>
                <span class="s1">_segmentcache.FetchStrategy.LoadingBoundary));</span>
                <span class="s2">if </span><span class="s1">(refetchMarkerContext !== </span><span class="s0">'refetch'</span><span class="s1">) {</span>
                    <span class="s1">refetchMarker = refetchMarkerContext = </span><span class="s0">'refetch'</span><span class="s1">;</span>
                <span class="s1">} </span><span class="s2">else </span><span class="s1">{</span>
                <span class="s4">// There's already a parent with a refetch marker, so we don't need</span>
                <span class="s4">// to add another one.</span>
                <span class="s1">}</span>
                <span class="s2">break</span><span class="s1">;</span>
            <span class="s1">}</span>
        <span class="s2">case </span><span class="s1">_cache.EntryStatus.Fulfilled:</span>
            <span class="s1">{</span>
                <span class="s4">// The segment is already cached.</span>
                <span class="s2">const </span><span class="s1">segmentHasLoadingBoundary = tree.hasLoadingBoundary === _types.HasLoadingBoundary.SegmentHasLoadingBoundary;</span>
                <span class="s2">if </span><span class="s1">(segmentHasLoadingBoundary) {</span>
                    <span class="s4">// This segment has a loading boundary, which means the server won't</span>
                    <span class="s4">// render its children. So there's nothing left to prefetch along this</span>
                    <span class="s4">// path. We can bail out.</span>
                    <span class="s2">return </span><span class="s1">(</span><span class="s3">0</span><span class="s1">, _cache.convertRouteTreeToFlightRouterState)(tree);</span>
                <span class="s1">}</span>
                <span class="s2">break</span><span class="s1">;</span>
            <span class="s1">}</span>
        <span class="s2">case </span><span class="s1">_cache.EntryStatus.Pending:</span>
            <span class="s1">{</span>
                <span class="s2">break</span><span class="s1">;</span>
            <span class="s1">}</span>
        <span class="s2">case </span><span class="s1">_cache.EntryStatus.Rejected:</span>
            <span class="s1">{</span>
                <span class="s2">break</span><span class="s1">;</span>
            <span class="s1">}</span>
        <span class="s2">default</span><span class="s1">:</span>
            <span class="s1">segment;</span>
    <span class="s1">}</span>
    <span class="s2">const </span><span class="s1">requestTreeChildren = {};</span>
    <span class="s2">if </span><span class="s1">(tree.slots !== </span><span class="s2">null</span><span class="s1">) {</span>
        <span class="s2">for</span><span class="s1">(</span><span class="s2">const </span><span class="s1">parallelRouteKey </span><span class="s2">in </span><span class="s1">tree.slots){</span>
            <span class="s2">const </span><span class="s1">childTree = tree.slots[parallelRouteKey];</span>
            <span class="s1">requestTreeChildren[parallelRouteKey] = pingPPRDisabledRouteTreeUpToLoadingBoundary(now, task, route, childTree, refetchMarkerContext, spawnedEntries);</span>
        <span class="s1">}</span>
    <span class="s1">}</span>
    <span class="s2">const </span><span class="s1">requestTree = [</span>
        <span class="s1">tree.segment,</span>
        <span class="s1">requestTreeChildren,</span>
        <span class="s2">null</span><span class="s1">,</span>
        <span class="s1">refetchMarker,</span>
        <span class="s1">tree.isRootLayout</span>
    <span class="s1">];</span>
    <span class="s2">return </span><span class="s1">requestTree;</span>
<span class="s1">}</span>
<span class="s2">function </span><span class="s1">pingRouteTreeAndIncludeDynamicData(now, task, route, tree, isInsideRefetchingParent, spawnedEntries, fetchStrategy) {</span>
    <span class="s4">// The tree we're constructing is the same shape as the tree we're navigating</span>
    <span class="s4">// to. But even though this is a &quot;new&quot; tree, some of the individual segments</span>
    <span class="s4">// may be cached as a result of other route prefetches.</span>
    <span class="s4">//</span>
    <span class="s4">// So we need to find the first uncached segment along each path add an</span>
    <span class="s4">// explicit &quot;refetch&quot; marker so the server knows where to start rendering.</span>
    <span class="s4">// Once the server starts rendering along a path, it keeps rendering the</span>
    <span class="s4">// entire subtree.</span>
    <span class="s2">const </span><span class="s1">segment = (</span><span class="s3">0</span><span class="s1">, _cache.readOrCreateSegmentCacheEntry)(now, task, route, tree.cacheKey);</span>
    <span class="s2">let </span><span class="s1">spawnedSegment = </span><span class="s2">null</span><span class="s1">;</span>
    <span class="s2">switch</span><span class="s1">(segment.status){</span>
        <span class="s2">case </span><span class="s1">_cache.EntryStatus.Empty:</span>
            <span class="s1">{</span>
                <span class="s4">// This segment is not cached. Include it in the request.</span>
                <span class="s1">spawnedSegment = (</span><span class="s3">0</span><span class="s1">, _cache.upgradeToPendingSegment)(segment, fetchStrategy);</span>
                <span class="s2">break</span><span class="s1">;</span>
            <span class="s1">}</span>
        <span class="s2">case </span><span class="s1">_cache.EntryStatus.Fulfilled:</span>
            <span class="s1">{</span>
                <span class="s4">// The segment is already cached.</span>
                <span class="s2">if </span><span class="s1">(segment.isPartial &amp;&amp; (</span><span class="s3">0</span><span class="s1">, _cache.canNewFetchStrategyProvideMoreContent)(segment.fetchStrategy, fetchStrategy)) {</span>
                    <span class="s4">// The cached segment contains dynamic holes, and was prefetched using a less specific strategy than the current one.</span>
                    <span class="s4">// This means we're in one of these cases:</span>
                    <span class="s4">//   - we have a static prefetch, and we're doing a runtime prefetch</span>
                    <span class="s4">//   - we have a static or runtime prefetch, and we're doing a Full prefetch (or a navigation).</span>
                    <span class="s4">// In either case, we need to include it in the request to get a more specific (or full) version.</span>
                    <span class="s1">spawnedSegment = pingFullSegmentRevalidation(now, task, route, segment, tree, fetchStrategy);</span>
                <span class="s1">}</span>
                <span class="s2">break</span><span class="s1">;</span>
            <span class="s1">}</span>
        <span class="s2">case </span><span class="s1">_cache.EntryStatus.Pending:</span>
        <span class="s2">case </span><span class="s1">_cache.EntryStatus.Rejected:</span>
            <span class="s1">{</span>
                <span class="s4">// There's either another prefetch currently in progress, or the previous</span>
                <span class="s4">// attempt failed. If the new strategy can provide more content, fetch it again.</span>
                <span class="s2">if </span><span class="s1">((</span><span class="s3">0</span><span class="s1">, _cache.canNewFetchStrategyProvideMoreContent)(segment.fetchStrategy, fetchStrategy)) {</span>
                    <span class="s1">spawnedSegment = pingFullSegmentRevalidation(now, task, route, segment, tree, fetchStrategy);</span>
                <span class="s1">}</span>
                <span class="s2">break</span><span class="s1">;</span>
            <span class="s1">}</span>
        <span class="s2">default</span><span class="s1">:</span>
            <span class="s1">segment;</span>
    <span class="s1">}</span>
    <span class="s2">const </span><span class="s1">requestTreeChildren = {};</span>
    <span class="s2">if </span><span class="s1">(tree.slots !== </span><span class="s2">null</span><span class="s1">) {</span>
        <span class="s2">for</span><span class="s1">(</span><span class="s2">const </span><span class="s1">parallelRouteKey </span><span class="s2">in </span><span class="s1">tree.slots){</span>
            <span class="s2">const </span><span class="s1">childTree = tree.slots[parallelRouteKey];</span>
            <span class="s1">requestTreeChildren[parallelRouteKey] = pingRouteTreeAndIncludeDynamicData(now, task, route, childTree, isInsideRefetchingParent || spawnedSegment !== </span><span class="s2">null</span><span class="s1">, spawnedEntries, fetchStrategy);</span>
        <span class="s1">}</span>
    <span class="s1">}</span>
    <span class="s2">if </span><span class="s1">(spawnedSegment !== </span><span class="s2">null</span><span class="s1">) {</span>
        <span class="s4">// Add the pending entry to the result map.</span>
        <span class="s1">spawnedEntries.set(tree.cacheKey, spawnedSegment);</span>
    <span class="s1">}</span>
    <span class="s4">// Don't bother to add a refetch marker if one is already present in a parent.</span>
    <span class="s2">const </span><span class="s1">refetchMarker = !isInsideRefetchingParent &amp;&amp; spawnedSegment !== </span><span class="s2">null </span><span class="s1">? </span><span class="s0">'refetch' </span><span class="s1">: </span><span class="s2">null</span><span class="s1">;</span>
    <span class="s2">const </span><span class="s1">requestTree = [</span>
        <span class="s1">tree.segment,</span>
        <span class="s1">requestTreeChildren,</span>
        <span class="s2">null</span><span class="s1">,</span>
        <span class="s1">refetchMarker,</span>
        <span class="s1">tree.isRootLayout</span>
    <span class="s1">];</span>
    <span class="s2">return </span><span class="s1">requestTree;</span>
<span class="s1">}</span>
<span class="s2">function </span><span class="s1">pingPerSegment(now, task, route, segment, routeKey, tree) {</span>
    <span class="s2">switch</span><span class="s1">(segment.status){</span>
        <span class="s2">case </span><span class="s1">_cache.EntryStatus.Empty:</span>
            <span class="s4">// Upgrade to Pending so we know there's already a request in progress</span>
            <span class="s1">spawnPrefetchSubtask((</span><span class="s3">0</span><span class="s1">, _cache.fetchSegmentOnCacheMiss)(route, (</span><span class="s3">0</span><span class="s1">, _cache.upgradeToPendingSegment)(segment, _segmentcache.FetchStrategy.PPR), routeKey, tree));</span>
            <span class="s2">break</span><span class="s1">;</span>
        <span class="s2">case </span><span class="s1">_cache.EntryStatus.Pending:</span>
            <span class="s1">{</span>
                <span class="s4">// There's already a request in progress. Depending on what kind of</span>
                <span class="s4">// request it is, we may want to revalidate it.</span>
                <span class="s2">switch</span><span class="s1">(segment.fetchStrategy){</span>
                    <span class="s2">case </span><span class="s1">_segmentcache.FetchStrategy.PPR:</span>
                    <span class="s2">case </span><span class="s1">_segmentcache.FetchStrategy.PPRRuntime:</span>
                    <span class="s2">case </span><span class="s1">_segmentcache.FetchStrategy.Full:</span>
                        <span class="s2">break</span><span class="s1">;</span>
                    <span class="s2">case </span><span class="s1">_segmentcache.FetchStrategy.LoadingBoundary:</span>
                        <span class="s4">// There's a pending request, but because it's using the old</span>
                        <span class="s4">// prefetching strategy, we can't be sure if it will be fulfilled by</span>
                        <span class="s4">// the response — it might be inside the loading boundary. Perform</span>
                        <span class="s4">// a revalidation, but because it's speculative, wait to do it at</span>
                        <span class="s4">// background priority.</span>
                        <span class="s2">if </span><span class="s1">(background(task)) {</span>
                            <span class="s4">// TODO: Instead of speculatively revalidating, consider including</span>
                            <span class="s4">// `hasLoading` in the route tree prefetch response.</span>
                            <span class="s1">pingPPRSegmentRevalidation(now, task, segment, route, routeKey, tree);</span>
                        <span class="s1">}</span>
                        <span class="s2">break</span><span class="s1">;</span>
                    <span class="s2">default</span><span class="s1">:</span>
                        <span class="s1">segment.fetchStrategy;</span>
                <span class="s1">}</span>
                <span class="s2">break</span><span class="s1">;</span>
            <span class="s1">}</span>
        <span class="s2">case </span><span class="s1">_cache.EntryStatus.Rejected:</span>
            <span class="s1">{</span>
                <span class="s4">// The existing entry in the cache was rejected. Depending on how it</span>
                <span class="s4">// was originally fetched, we may or may not want to revalidate it.</span>
                <span class="s2">switch</span><span class="s1">(segment.fetchStrategy){</span>
                    <span class="s2">case </span><span class="s1">_segmentcache.FetchStrategy.PPR:</span>
                    <span class="s2">case </span><span class="s1">_segmentcache.FetchStrategy.PPRRuntime:</span>
                    <span class="s2">case </span><span class="s1">_segmentcache.FetchStrategy.Full:</span>
                        <span class="s2">break</span><span class="s1">;</span>
                    <span class="s2">case </span><span class="s1">_segmentcache.FetchStrategy.LoadingBoundary:</span>
                        <span class="s4">// There's a rejected entry, but it was fetched using the loading</span>
                        <span class="s4">// boundary strategy. So the reason it wasn't returned by the server</span>
                        <span class="s4">// might just be because it was inside a loading boundary. Or because</span>
                        <span class="s4">// there was a dynamic rewrite. Revalidate it using the per-</span>
                        <span class="s4">// segment strategy.</span>
                        <span class="s4">//</span>
                        <span class="s4">// Because a rejected segment will definitely prevent the segment (and</span>
                        <span class="s4">// all of its children) from rendering, we perform this revalidation</span>
                        <span class="s4">// immediately instead of deferring it to a background task.</span>
                        <span class="s1">pingPPRSegmentRevalidation(now, task, segment, route, routeKey, tree);</span>
                        <span class="s2">break</span><span class="s1">;</span>
                    <span class="s2">default</span><span class="s1">:</span>
                        <span class="s1">segment.fetchStrategy;</span>
                <span class="s1">}</span>
                <span class="s2">break</span><span class="s1">;</span>
            <span class="s1">}</span>
        <span class="s2">case </span><span class="s1">_cache.EntryStatus.Fulfilled:</span>
            <span class="s2">break</span><span class="s1">;</span>
        <span class="s2">default</span><span class="s1">:</span>
            <span class="s1">segment;</span>
    <span class="s1">}</span>
<span class="s4">// Segments do not have dependent tasks, so once the prefetch is initiated,</span>
<span class="s4">// there's nothing else for us to do (except write the server data into the</span>
<span class="s4">// entry, which is handled by `fetchSegmentOnCacheMiss`).</span>
<span class="s1">}</span>
<span class="s2">function </span><span class="s1">pingPPRSegmentRevalidation(now, task, currentSegment, route, routeKey, tree) {</span>
    <span class="s2">const </span><span class="s1">revalidatingSegment = (</span><span class="s3">0</span><span class="s1">, _cache.readOrCreateRevalidatingSegmentEntry)(now, currentSegment);</span>
    <span class="s2">switch</span><span class="s1">(revalidatingSegment.status){</span>
        <span class="s2">case </span><span class="s1">_cache.EntryStatus.Empty:</span>
            <span class="s4">// Spawn a prefetch request and upsert the segment into the cache</span>
            <span class="s4">// upon completion.</span>
            <span class="s1">upsertSegmentOnCompletion(task, route, tree.cacheKey, spawnPrefetchSubtask((</span><span class="s3">0</span><span class="s1">, _cache.fetchSegmentOnCacheMiss)(route, (</span><span class="s3">0</span><span class="s1">, _cache.upgradeToPendingSegment)(revalidatingSegment, _segmentcache.FetchStrategy.PPR), routeKey, tree)));</span>
            <span class="s2">break</span><span class="s1">;</span>
        <span class="s2">case </span><span class="s1">_cache.EntryStatus.Pending:</span>
            <span class="s2">break</span><span class="s1">;</span>
        <span class="s2">case </span><span class="s1">_cache.EntryStatus.Fulfilled:</span>
        <span class="s2">case </span><span class="s1">_cache.EntryStatus.Rejected:</span>
            <span class="s2">break</span><span class="s1">;</span>
        <span class="s2">default</span><span class="s1">:</span>
            <span class="s1">revalidatingSegment;</span>
    <span class="s1">}</span>
<span class="s1">}</span>
<span class="s2">function </span><span class="s1">pingFullSegmentRevalidation(now, task, route, currentSegment, tree, fetchStrategy) {</span>
    <span class="s2">const </span><span class="s1">revalidatingSegment = (</span><span class="s3">0</span><span class="s1">, _cache.readOrCreateRevalidatingSegmentEntry)(now, currentSegment);</span>
    <span class="s2">if </span><span class="s1">(revalidatingSegment.status === _cache.EntryStatus.Empty) {</span>
        <span class="s4">// During a Full/PPRRuntime prefetch, a single dynamic request is made for all the</span>
        <span class="s4">// segments that we need. So we don't initiate a request here directly. By</span>
        <span class="s4">// returning a pending entry from this function, it signals to the caller</span>
        <span class="s4">// that this segment should be included in the request that's sent to</span>
        <span class="s4">// the server.</span>
        <span class="s2">const </span><span class="s1">pendingSegment = (</span><span class="s3">0</span><span class="s1">, _cache.upgradeToPendingSegment)(revalidatingSegment, fetchStrategy);</span>
        <span class="s1">upsertSegmentOnCompletion(task, route, tree.cacheKey, (</span><span class="s3">0</span><span class="s1">, _cache.waitForSegmentCacheEntry)(pendingSegment));</span>
        <span class="s2">return </span><span class="s1">pendingSegment;</span>
    <span class="s1">} </span><span class="s2">else </span><span class="s1">{</span>
        <span class="s4">// There's already a revalidation in progress.</span>
        <span class="s2">const </span><span class="s1">nonEmptyRevalidatingSegment = revalidatingSegment;</span>
        <span class="s2">if </span><span class="s1">((</span><span class="s3">0</span><span class="s1">, _cache.canNewFetchStrategyProvideMoreContent)(nonEmptyRevalidatingSegment.fetchStrategy, fetchStrategy)) {</span>
            <span class="s4">// The existing revalidation was fetched using a less specific strategy.</span>
            <span class="s4">// Reset it and start a new revalidation.</span>
            <span class="s2">const </span><span class="s1">emptySegment = (</span><span class="s3">0</span><span class="s1">, _cache.resetRevalidatingSegmentEntry)(nonEmptyRevalidatingSegment);</span>
            <span class="s2">const </span><span class="s1">pendingSegment = (</span><span class="s3">0</span><span class="s1">, _cache.upgradeToPendingSegment)(emptySegment, fetchStrategy);</span>
            <span class="s1">upsertSegmentOnCompletion(task, route, tree.cacheKey, (</span><span class="s3">0</span><span class="s1">, _cache.waitForSegmentCacheEntry)(pendingSegment));</span>
            <span class="s2">return </span><span class="s1">pendingSegment;</span>
        <span class="s1">}</span>
        <span class="s2">switch</span><span class="s1">(nonEmptyRevalidatingSegment.status){</span>
            <span class="s2">case </span><span class="s1">_cache.EntryStatus.Pending:</span>
                <span class="s4">// There's already an in-progress prefetch that includes this segment.</span>
                <span class="s2">return null</span><span class="s1">;</span>
            <span class="s2">case </span><span class="s1">_cache.EntryStatus.Fulfilled:</span>
            <span class="s2">case </span><span class="s1">_cache.EntryStatus.Rejected:</span>
                <span class="s4">// A previous revalidation attempt finished, but we chose not to replace</span>
                <span class="s4">// the existing entry in the cache. Don't try again until or unless the</span>
                <span class="s4">// revalidation entry expires.</span>
                <span class="s2">return null</span><span class="s1">;</span>
            <span class="s2">default</span><span class="s1">:</span>
                <span class="s1">nonEmptyRevalidatingSegment;</span>
                <span class="s2">return null</span><span class="s1">;</span>
        <span class="s1">}</span>
    <span class="s1">}</span>
<span class="s1">}</span>
<span class="s2">const </span><span class="s1">noop = ()=&gt;{};</span>
<span class="s2">function </span><span class="s1">upsertSegmentOnCompletion(task, route, cacheKey, promise) {</span>
    <span class="s4">// Wait for a segment to finish loading, then upsert it into the cache</span>
    <span class="s1">promise.then((fulfilled)=&gt;{</span>
        <span class="s2">if </span><span class="s1">(fulfilled !== </span><span class="s2">null</span><span class="s1">) {</span>
            <span class="s4">// Received new data. Attempt to replace the existing entry in the cache.</span>
            <span class="s2">const </span><span class="s1">keypath = (</span><span class="s3">0</span><span class="s1">, _cache.getSegmentKeypathForTask)(task, route, cacheKey);</span>
            <span class="s1">(</span><span class="s3">0</span><span class="s1">, _cache.upsertSegmentEntry)(Date.now(), keypath, fulfilled);</span>
        <span class="s1">}</span>
    <span class="s1">}, noop);</span>
<span class="s1">}</span>
<span class="s2">function </span><span class="s1">doesCurrentSegmentMatchCachedSegment(route, currentSegment, cachedSegment) {</span>
    <span class="s2">if </span><span class="s1">(cachedSegment === _segment.PAGE_SEGMENT_KEY) {</span>
        <span class="s4">// In the FlightRouterState stored by the router, the page segment has the</span>
        <span class="s4">// rendered search params appended to the name of the segment. In the</span>
        <span class="s4">// prefetch cache, however, this is stored separately. So, when comparing</span>
        <span class="s4">// the router's current FlightRouterState to the cached FlightRouterState,</span>
        <span class="s4">// we need to make sure we compare both parts of the segment.</span>
        <span class="s4">// TODO: This is not modeled clearly. We use the same type,</span>
        <span class="s4">// FlightRouterState, for both the CacheNode tree _and_ the prefetch cache</span>
        <span class="s4">// _and_ the server response format, when conceptually those are three</span>
        <span class="s4">// different things and treated in different ways. We should encode more of</span>
        <span class="s4">// this information into the type design so mistakes are less likely.</span>
        <span class="s2">return </span><span class="s1">currentSegment === (</span><span class="s3">0</span><span class="s1">, _segment.addSearchParamsIfPageSegment)(_segment.PAGE_SEGMENT_KEY, Object.fromEntries(</span><span class="s2">new </span><span class="s1">URLSearchParams(route.renderedSearch)));</span>
    <span class="s1">}</span>
    <span class="s4">// Non-page segments are compared using the same function as the server</span>
    <span class="s2">return </span><span class="s1">(</span><span class="s3">0</span><span class="s1">, _matchsegments.matchSegment)(cachedSegment, currentSegment);</span>
<span class="s1">}</span>
<span class="s4">// -----------------------------------------------------------------------------</span>
<span class="s4">// The remainder of the module is a MinHeap implementation. Try not to put any</span>
<span class="s4">// logic below here unless it's related to the heap algorithm. We can extract</span>
<span class="s4">// this to a separate module if/when we need multiple kinds of heaps.</span>
<span class="s4">// -----------------------------------------------------------------------------</span>
<span class="s2">function </span><span class="s1">compareQueuePriority(a, b) {</span>
    <span class="s4">// Since the queue is a MinHeap, this should return a positive number if b is</span>
    <span class="s4">// higher priority than a, and a negative number if a is higher priority</span>
    <span class="s4">// than b.</span>
    <span class="s4">// `priority` is an integer, where higher numbers are higher priority.</span>
    <span class="s2">const </span><span class="s1">priorityDiff = b.priority - a.priority;</span>
    <span class="s2">if </span><span class="s1">(priorityDiff !== </span><span class="s3">0</span><span class="s1">) {</span>
        <span class="s2">return </span><span class="s1">priorityDiff;</span>
    <span class="s1">}</span>
    <span class="s4">// If the priority is the same, check which phase the prefetch is in — is it</span>
    <span class="s4">// prefetching the route tree, or the segments? Route trees are prioritized.</span>
    <span class="s2">const </span><span class="s1">phaseDiff = b.phase - a.phase;</span>
    <span class="s2">if </span><span class="s1">(phaseDiff !== </span><span class="s3">0</span><span class="s1">) {</span>
        <span class="s2">return </span><span class="s1">phaseDiff;</span>
    <span class="s1">}</span>
    <span class="s4">// Finally, check the insertion order. `sortId` is an incrementing counter</span>
    <span class="s4">// assigned to prefetches. We want to process the newest prefetches first.</span>
    <span class="s2">return </span><span class="s1">b.sortId - a.sortId;</span>
<span class="s1">}</span>
<span class="s2">function </span><span class="s1">heapPush(heap, node) {</span>
    <span class="s2">const </span><span class="s1">index = heap.length;</span>
    <span class="s1">heap.push(node);</span>
    <span class="s1">node._heapIndex = index;</span>
    <span class="s1">heapSiftUp(heap, node, index);</span>
<span class="s1">}</span>
<span class="s2">function </span><span class="s1">heapPeek(heap) {</span>
    <span class="s2">return </span><span class="s1">heap.length === </span><span class="s3">0 </span><span class="s1">? </span><span class="s2">null </span><span class="s1">: heap[</span><span class="s3">0</span><span class="s1">];</span>
<span class="s1">}</span>
<span class="s2">function </span><span class="s1">heapPop(heap) {</span>
    <span class="s2">if </span><span class="s1">(heap.length === </span><span class="s3">0</span><span class="s1">) {</span>
        <span class="s2">return null</span><span class="s1">;</span>
    <span class="s1">}</span>
    <span class="s2">const </span><span class="s1">first = heap[</span><span class="s3">0</span><span class="s1">];</span>
    <span class="s1">first._heapIndex = -</span><span class="s3">1</span><span class="s1">;</span>
    <span class="s2">const </span><span class="s1">last = heap.pop();</span>
    <span class="s2">if </span><span class="s1">(last !== first) {</span>
        <span class="s1">heap[</span><span class="s3">0</span><span class="s1">] = last;</span>
        <span class="s1">last._heapIndex = </span><span class="s3">0</span><span class="s1">;</span>
        <span class="s1">heapSiftDown(heap, last, </span><span class="s3">0</span><span class="s1">);</span>
    <span class="s1">}</span>
    <span class="s2">return </span><span class="s1">first;</span>
<span class="s1">}</span>
<span class="s2">function </span><span class="s1">heapDelete(heap, node) {</span>
    <span class="s2">const </span><span class="s1">index = node._heapIndex;</span>
    <span class="s2">if </span><span class="s1">(index !== -</span><span class="s3">1</span><span class="s1">) {</span>
        <span class="s1">node._heapIndex = -</span><span class="s3">1</span><span class="s1">;</span>
        <span class="s2">if </span><span class="s1">(heap.length !== </span><span class="s3">0</span><span class="s1">) {</span>
            <span class="s2">const </span><span class="s1">last = heap.pop();</span>
            <span class="s2">if </span><span class="s1">(last !== node) {</span>
                <span class="s1">heap[index] = last;</span>
                <span class="s1">last._heapIndex = index;</span>
                <span class="s1">heapSiftDown(heap, last, index);</span>
            <span class="s1">}</span>
        <span class="s1">}</span>
    <span class="s1">}</span>
<span class="s1">}</span>
<span class="s2">function </span><span class="s1">heapResift(heap, node) {</span>
    <span class="s2">const </span><span class="s1">index = node._heapIndex;</span>
    <span class="s2">if </span><span class="s1">(index !== -</span><span class="s3">1</span><span class="s1">) {</span>
        <span class="s2">if </span><span class="s1">(index === </span><span class="s3">0</span><span class="s1">) {</span>
            <span class="s1">heapSiftDown(heap, node, </span><span class="s3">0</span><span class="s1">);</span>
        <span class="s1">} </span><span class="s2">else </span><span class="s1">{</span>
            <span class="s2">const </span><span class="s1">parentIndex = index - </span><span class="s3">1 </span><span class="s1">&gt;&gt;&gt; </span><span class="s3">1</span><span class="s1">;</span>
            <span class="s2">const </span><span class="s1">parent = heap[parentIndex];</span>
            <span class="s2">if </span><span class="s1">(compareQueuePriority(parent, node) &gt; </span><span class="s3">0</span><span class="s1">) {</span>
                <span class="s4">// The parent is larger. Sift up.</span>
                <span class="s1">heapSiftUp(heap, node, index);</span>
            <span class="s1">} </span><span class="s2">else </span><span class="s1">{</span>
                <span class="s4">// The parent is smaller (or equal). Sift down.</span>
                <span class="s1">heapSiftDown(heap, node, index);</span>
            <span class="s1">}</span>
        <span class="s1">}</span>
    <span class="s1">}</span>
<span class="s1">}</span>
<span class="s2">function </span><span class="s1">heapSiftUp(heap, node, i) {</span>
    <span class="s2">let </span><span class="s1">index = i;</span>
    <span class="s2">while</span><span class="s1">(index &gt; </span><span class="s3">0</span><span class="s1">){</span>
        <span class="s2">const </span><span class="s1">parentIndex = index - </span><span class="s3">1 </span><span class="s1">&gt;&gt;&gt; </span><span class="s3">1</span><span class="s1">;</span>
        <span class="s2">const </span><span class="s1">parent = heap[parentIndex];</span>
        <span class="s2">if </span><span class="s1">(compareQueuePriority(parent, node) &gt; </span><span class="s3">0</span><span class="s1">) {</span>
            <span class="s4">// The parent is larger. Swap positions.</span>
            <span class="s1">heap[parentIndex] = node;</span>
            <span class="s1">node._heapIndex = parentIndex;</span>
            <span class="s1">heap[index] = parent;</span>
            <span class="s1">parent._heapIndex = index;</span>
            <span class="s1">index = parentIndex;</span>
        <span class="s1">} </span><span class="s2">else </span><span class="s1">{</span>
            <span class="s4">// The parent is smaller. Exit.</span>
            <span class="s2">return</span><span class="s1">;</span>
        <span class="s1">}</span>
    <span class="s1">}</span>
<span class="s1">}</span>
<span class="s2">function </span><span class="s1">heapSiftDown(heap, node, i) {</span>
    <span class="s2">let </span><span class="s1">index = i;</span>
    <span class="s2">const </span><span class="s1">length = heap.length;</span>
    <span class="s2">const </span><span class="s1">halfLength = length &gt;&gt;&gt; </span><span class="s3">1</span><span class="s1">;</span>
    <span class="s2">while</span><span class="s1">(index &lt; halfLength){</span>
        <span class="s2">const </span><span class="s1">leftIndex = (index + </span><span class="s3">1</span><span class="s1">) * </span><span class="s3">2 </span><span class="s1">- </span><span class="s3">1</span><span class="s1">;</span>
        <span class="s2">const </span><span class="s1">left = heap[leftIndex];</span>
        <span class="s2">const </span><span class="s1">rightIndex = leftIndex + </span><span class="s3">1</span><span class="s1">;</span>
        <span class="s2">const </span><span class="s1">right = heap[rightIndex];</span>
        <span class="s4">// If the left or right node is smaller, swap with the smaller of those.</span>
        <span class="s2">if </span><span class="s1">(compareQueuePriority(left, node) &lt; </span><span class="s3">0</span><span class="s1">) {</span>
            <span class="s2">if </span><span class="s1">(rightIndex &lt; length &amp;&amp; compareQueuePriority(right, left) &lt; </span><span class="s3">0</span><span class="s1">) {</span>
                <span class="s1">heap[index] = right;</span>
                <span class="s1">right._heapIndex = index;</span>
                <span class="s1">heap[rightIndex] = node;</span>
                <span class="s1">node._heapIndex = rightIndex;</span>
                <span class="s1">index = rightIndex;</span>
            <span class="s1">} </span><span class="s2">else </span><span class="s1">{</span>
                <span class="s1">heap[index] = left;</span>
                <span class="s1">left._heapIndex = index;</span>
                <span class="s1">heap[leftIndex] = node;</span>
                <span class="s1">node._heapIndex = leftIndex;</span>
                <span class="s1">index = leftIndex;</span>
            <span class="s1">}</span>
        <span class="s1">} </span><span class="s2">else if </span><span class="s1">(rightIndex &lt; length &amp;&amp; compareQueuePriority(right, node) &lt; </span><span class="s3">0</span><span class="s1">) {</span>
            <span class="s1">heap[index] = right;</span>
            <span class="s1">right._heapIndex = index;</span>
            <span class="s1">heap[rightIndex] = node;</span>
            <span class="s1">node._heapIndex = rightIndex;</span>
            <span class="s1">index = rightIndex;</span>
        <span class="s1">} </span><span class="s2">else </span><span class="s1">{</span>
            <span class="s4">// Neither child is smaller. Exit.</span>
            <span class="s2">return</span><span class="s1">;</span>
        <span class="s1">}</span>
    <span class="s1">}</span>
<span class="s1">}</span>

<span class="s2">if </span><span class="s1">((</span><span class="s2">typeof </span><span class="s1">exports.default === </span><span class="s0">'function' </span><span class="s1">|| (</span><span class="s2">typeof </span><span class="s1">exports.default === </span><span class="s0">'object' </span><span class="s1">&amp;&amp; exports.default !== </span><span class="s2">null</span><span class="s1">)) &amp;&amp; </span><span class="s2">typeof </span><span class="s1">exports.default.__esModule === </span><span class="s0">'undefined'</span><span class="s1">) {</span>
  <span class="s1">Object.defineProperty(exports.default, </span><span class="s0">'__esModule'</span><span class="s1">, { value: </span><span class="s2">true </span><span class="s1">});</span>
  <span class="s1">Object.assign(exports.default, exports);</span>
  <span class="s1">module.exports = exports.default;</span>
<span class="s1">}</span>

<span class="s4">//# sourceMappingURL=scheduler.js.map</span></pre>
</body>
</html>